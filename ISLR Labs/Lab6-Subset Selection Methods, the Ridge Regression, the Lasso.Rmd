---
title: "Lab 6-Subset Selection Methods, the Ridge Regression, the Lasso"
output: html_document
---
```{r}
install.packages('ISLR')
install.packages('leaps')
install.packages('glmnet')
```

```{r}
library(ISLR)
library(leaps)
library(glmnet)
```

# Subset Selection Methods
## Best Subset Selection
```{r}
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
# remove NAs
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
regfit.full = regsubsets(Salary ~ ., Hitters)
summary(regfit.full)
# specify maximum number of variables
regfit.full = regsubsets(Salary ~ ., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)

# plot RSS, adjusted R-square
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq")

# identify the point with maximum adjusted R-square
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)

# plot Cp and BIC and identify the point with minimum value
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)

plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC")
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)

# display the selected variables for the best model with a given number of predictors ranked according to r2, adjr2, Cp, BIC
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
```

## Forward and Backward Stepwise Selection
```{r}
# Forward Stepwise Selection
regfit.fwd = regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)

# Backward Stepwise Selection
regfit.bwd = regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
```

## Choosing Among Models using the Validation Set and Cross-Validation
### Validation Set
```{r}
# split into training/test set
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep=TRUE)
test = (!train)

# perform best subset selection
regfit.best = regsubsets(Salary ~ ., data=Hitters[train,], nvmax=19)

# make a model matrix from test set
test.mat = model.matrix(Salary ~ ., data=Hitters[test,])
# loop to calculate test MSE
val.errors = rep(NA, 19)
for (i in 1:19){
  coefi = coef(regfit.best, id=i)
  pred = test.mat[,names(coefi)]%*%coefi
  val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
}

# find the best model
which.min(val.errors)
coef(regfit.best, 10)

# use full data set to obtain more accurate coefficient estimates
regfit.best = regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(regfit.best, 10)
```

### Cross-Validation
```{r}
k=10
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace=TRUE)
cv.errors = matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))

# prediction function
predict.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id=id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}

for (j in 1:k){
  best.fit = regsubsets(Salary ~ ., data=Hitters[folds!=j,], nvmax=19)
  for (i in 1:19){                      
    pred = predict(best.fit, Hitters[folds==j,], id=1)
    cv.errors[j,i] = mean((Hitters$Salary[folds==j]-pred)^2)
    }
}

# find the best model
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type='b')

# use full data set to obtain more accurate coefficient estimates
reg.best = regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(reg.best, 11)
```

# Ridge Regression and the Lasso
## Ridge Regression
```{r}
x = model.matrix(Salary ~ ., Hitters)[,-1]
y = Hitters$Salary

grid = 10^seq(10, -2, length=100)
ridge.mod = glmnet(x, y, alpha=0, lambda=grid)
predict(ridge.mod, s=50, type="coefficients")[1:20,]

# split the train/test set
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)
ridge.pred = predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
```









