---
title: "Lab 6-Subset Selection Methods, the Ridge Regression, the Lasso"
output: html_document
---
```{r}
install.packages('ISLR')
install.packages('leaps')
install.packages('glmnet')
install.packages('pls')
```

```{r}
library(ISLR)
library(leaps)
library(glmnet)
library(pls)
```

# Subset Selection Methods
## Best Subset Selection
```{r}
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
# remove NAs
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
regfit.full = regsubsets(Salary ~ ., Hitters)
summary(regfit.full)
# specify maximum number of variables
regfit.full = regsubsets(Salary ~ ., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)

# plot RSS, adjusted R-square
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab="Number of Variables", ylab="RSS")
plot(reg.summary$adjr2, xlab="Number of Variables", ylab="Adjusted RSq")

# identify the point with maximum adjusted R-square
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col="red", cex=2, pch=20)

# plot Cp and BIC and identify the point with minimum value
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col="red", cex=2, pch=20)

plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC")
points(6, reg.summary$bic[6], col="red", cex=2, pch=20)

# display the selected variables for the best model with a given number of predictors ranked according to r2, adjr2, Cp, BIC
plot(regfit.full, scale="r2")
plot(regfit.full, scale="adjr2")
plot(regfit.full, scale="Cp")
plot(regfit.full, scale="bic")
```

## Forward and Backward Stepwise Selection
```{r}
# Forward Stepwise Selection
regfit.fwd = regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="forward")
summary(regfit.fwd)

# Backward Stepwise Selection
regfit.bwd = regsubsets(Salary ~ ., data=Hitters, nvmax=19, method="backward")
summary(regfit.bwd)
```

## Choosing Among Models using the Validation Set and Cross-Validation
### Validation Set
```{r}
# split into training/test set
set.seed(1)
train = sample(c(TRUE, FALSE), nrow(Hitters), rep=TRUE)
test = (!train)

# perform best subset selection
regfit.best = regsubsets(Salary ~ ., data=Hitters[train,], nvmax=19)

# make a model matrix from test set
test.mat = model.matrix(Salary ~ ., data=Hitters[test,])
# loop to calculate test MSE
val.errors = rep(NA, 19)
for (i in 1:19){
  coefi = coef(regfit.best, id=i)
  pred = test.mat[,names(coefi)]%*%coefi
  val.errors[i] = mean((Hitters$Salary[test] - pred)^2)
}

# find the best model
which.min(val.errors)
coef(regfit.best, 10)

# use full data set to obtain more accurate coefficient estimates
regfit.best = regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(regfit.best, 10)
```

### Cross-Validation
```{r}
k=10
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace=TRUE)
cv.errors = matrix(NA, k, 19, dimnames=list(NULL, paste(1:19)))

# prediction function
predict.regsubsets = function(object, newdata, id, ...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object, id=id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}

for (j in 1:k){
  best.fit = regsubsets(Salary ~ ., data=Hitters[folds!=j,], nvmax=19)
  for (i in 1:19){                      
    pred = predict(best.fit, Hitters[folds==j,], id=1)
    cv.errors[j,i] = mean((Hitters$Salary[folds==j]-pred)^2)
    }
}

# find the best model
mean.cv.errors = apply(cv.errors, 2, mean)
mean.cv.errors
par(mfrow = c(1,1))
plot(mean.cv.errors, type='b')

# use full data set to obtain more accurate coefficient estimates
reg.best = regsubsets(Salary ~ ., data=Hitters, nvmax=19)
coef(reg.best, 11)
```

# Ridge Regression and the Lasso
## Ridge Regression
```{r}
x = model.matrix(Salary ~ ., Hitters)[,-1]
y = Hitters$Salary

# the null model with lambda=50
grid = 10^seq(10, -2, length=100)
ridge.mod = glmnet(x, y, alpha=0, lambda=grid)
predict(ridge.mod, s=50, type="coefficients")[1:20,]

# split the train/test set
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = (-train)
y.test = y[test]
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda=grid, thresh=1e-12)

# cross-validation to obtain lambda that produces the smallest cross validation error
cv.out = cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam = cv.out$lambda.min
ridge.pred = predict(ridge.mod, s=bestlam, newx=x[test,])

# test MSE
mean((ridge.pred-y.test)^2)

# refit the ridge regression model on the full data set
out = glmnet(x, y, alpha=0)
predict(out, type="coefficients", s=bestlam)[1:20,]

```
## Lasso
```{r}
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
plot(lasso.mod)

# cross validation
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s=bestlam, newx=x[test,])

# test MSE
mean((lasso.pred - y.test)^2)

# fit the full data
out = glmnet(x, y , alpha=1, lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:20,]
lasso.coef
```

# PCR and PLS Regression
## PCR
```{r}
set.seed(2)
pcr.fit = pcr(Salary ~ ., data=Hitters, scale=TRUE, validation="CV")
summary(pcr.fit)

# plot cross-validation MSEs
validationplot(pcr.fit, val.type="MSEP")

# perform PCR on the training data and evaluate its test set performance
set.seed(1)
pcr.fit = pcr(Salary ~ ., data=Hitters, subset=train, scale=TRUE, validation="CV")
validationplot(pcr.fit, val.type="MSEP")
# the plot tells that the lowest cross-validation error occurs when M = 7 component are used

# test MSE
pcr.pred = predict(pcr.fit, x[test,], ncomp=7)
mean((pcr.pred - y.test)^2)

# fit PCR on the full data set
pcr.fit = pcr(y ~ x, scale=TRUE, ncomp=7)
summary(pcr.fit)
```

## PLS
```{r}
# perform PLS on the training data
set.seed(1)
pls.fit = plsr(Salary ~ ., data=Hitters, subset=train, scale=TRUE, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type="MSEP")
# the lowest cross-validation error occurs when M = 2

# test MSE
pls.pred = predict(pls.fit, x[test,], ncomp=2)
mean((pls.pred - y.test)^2)

# perform PLS on the full data set
pls.fit = plsr(Salary ~ ., data=Hitters, scale=TRUE, ncomp=2)
summary(pls.fit)
```




